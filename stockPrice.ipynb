{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.5-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37564bitd1e0ee96de8d4a6e831de06421d25b82",
   "display_name": "Python 3.7.5 64-bit"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Warning - Certain functionality \n             requires requests_html, which is not installed.\n             \n             Install using: \n             pip install requests_html\n             \n             After installation, you may have to restart your Python session.\n"
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from yahoo_fin import stock_info as si\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(ticker, n_steps=50, scale=True, shuffle=True, lookup_step=1, \n",
    "                test_size=0.2, feature_columns=['adjclose', 'volume', 'open', 'high', 'low']):\n",
    "    # see if ticker is already a loaded stock from yahoo finance\n",
    "    if isinstance(ticker, str):\n",
    "        # load it from yahoo_fin library\n",
    "        df = si.get_data(ticker)\n",
    "    elif isinstance(ticker, pd.DataFrame):\n",
    "        # already loaded, use it directly\n",
    "        df = ticker\n",
    "    # this will contain all the elements we want to return from this function\n",
    "    result = {}\n",
    "    # we will also return the original dataframe itself\n",
    "    result['df'] = df.copy()\n",
    "    # make sure that the passed feature_columns exist in the dataframe\n",
    "    for col in feature_columns:\n",
    "        assert col in df.columns\n",
    "    if scale:\n",
    "        column_scaler = {}\n",
    "        # scale the data (prices) from 0 to 1\n",
    "        for column in feature_columns:\n",
    "            scaler = preprocessing.MinMaxScaler()\n",
    "            df[column] = scaler.fit_transform(np.expand_dims(df[column].values, axis=1))\n",
    "            column_scaler[column] = scaler\n",
    "\n",
    "        # add the MinMaxScaler instances to the result returned\n",
    "        result[\"column_scaler\"] = column_scaler\n",
    "    # add the target column (label) by shifting by `lookup_step`\n",
    "    df['future'] = df['adjclose'].shift(-lookup_step)\n",
    "    # last `lookup_step` columns contains NaN in future column\n",
    "    # get them before droping NaNs\n",
    "    last_sequence = np.array(df[feature_columns].tail(lookup_step))\n",
    "    # drop NaNs\n",
    "    df.dropna(inplace=True)\n",
    "    sequence_data = []\n",
    "    sequences = deque(maxlen=n_steps)\n",
    "    for entry, target in zip(df[feature_columns].values, df['future'].values):\n",
    "        sequences.append(entry)\n",
    "        if len(sequences) == n_steps:\n",
    "            sequence_data.append([np.array(sequences), target])\n",
    "    # get the last sequence by appending the last `n_step` sequence with `lookup_step` sequence\n",
    "    # for instance, if n_steps=50 and lookup_step=10, last_sequence should be of 59 (that is 50+10-1) length\n",
    "    # this last_sequence will be used to predict in future dates that are not available in the dataset\n",
    "    last_sequence = list(sequences) + list(last_sequence)\n",
    "    # shift the last sequence by -1\n",
    "    last_sequence = np.array(pd.DataFrame(last_sequence).shift(-1).dropna())\n",
    "    # add to result\n",
    "    result['last_sequence'] = last_sequence\n",
    "    # construct the X's and y's\n",
    "    X, y = [], []\n",
    "    for seq, target in sequence_data:\n",
    "        X.append(seq)\n",
    "        y.append(target)\n",
    "    # convert to numpy arrays\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    # reshape X to fit the neural network\n",
    "    X = X.reshape((X.shape[0], X.shape[2], X.shape[1]))\n",
    "    # split the dataset\n",
    "    result[\"X_train\"], result[\"X_test\"], result[\"y_train\"], result[\"y_test\"] = train_test_split(X, y, test_size=test_size, shuffle=shuffle)\n",
    "    # return the result\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "{'df':                   open        high         low       close    adjclose  \\\n 2010-06-29   19.000000   25.000000   17.540001   23.889999   23.889999   \n 2010-06-30   25.790001   30.420000   23.299999   23.830000   23.830000   \n 2010-07-01   25.000000   25.920000   20.270000   21.959999   21.959999   \n 2010-07-02   23.000000   23.100000   18.709999   19.200001   19.200001   \n 2010-07-06   20.000000   20.000000   15.830000   16.110001   16.110001   \n ...                ...         ...         ...         ...         ...   \n 2020-02-12  777.869995  789.750000  763.369995  767.289978  767.289978   \n 2020-02-13  741.840027  818.000000  735.000000  804.000000  804.000000   \n 2020-02-14  787.219971  812.969971  785.500000  800.030029  800.030029   \n 2020-02-18  841.599976  860.000000  832.359985  858.400024  858.400024   \n 2020-02-19  923.500000  944.780029  901.020020  917.419983  917.419983   \n \n               volume ticker  \n 2010-06-29  18766300   TSLA  \n 2010-06-30  17187100   TSLA  \n 2010-07-01   8218800   TSLA  \n 2010-07-02   5139800   TSLA  \n 2010-07-06   6866900   TSLA  \n ...              ...    ...  \n 2020-02-12  12022500   TSLA  \n 2020-02-13  26289300   TSLA  \n 2020-02-14  15693700   TSLA  \n 2020-02-18  16381700   TSLA  \n 2020-02-19  25304300   TSLA  \n \n [2427 rows x 7 columns],\n 'column_scaler': {'adjclose': MinMaxScaler(copy=True, feature_range=(0, 1)),\n  'volume': MinMaxScaler(copy=True, feature_range=(0, 1)),\n  'open': MinMaxScaler(copy=True, feature_range=(0, 1)),\n  'high': MinMaxScaler(copy=True, feature_range=(0, 1)),\n  'low': MinMaxScaler(copy=True, feature_range=(0, 1))},\n 'last_sequence': array([[0.35501655, 0.1232138 , 0.35141509, 0.33834893, 0.36092048],\n        [0.35905371, 0.14640835, 0.35316743, 0.34421859, 0.36127035],\n        [0.36937956, 0.14320548, 0.35688149, 0.35081273, 0.36604441],\n        [0.37366076, 0.11146443, 0.37001852, 0.35759587, 0.37933952],\n        [0.38140236, 0.12570474, 0.37336891, 0.36342349, 0.38175478],\n        [0.37997163, 0.10608958, 0.38012474, 0.36601705, 0.38334613],\n        [0.40560326, 0.29686963, 0.38177789, 0.38533747, 0.39221705],\n        [0.40281937, 0.13775499, 0.39989639, 0.38732203, 0.40734051],\n        [0.41852443, 0.23022741, 0.40170385, 0.39752825, 0.41262243],\n        [0.43060271, 0.29576638, 0.42009787, 0.40974003, 0.43059003],\n        [0.43232182, 0.24061374, 0.43439209, 0.41619766, 0.43475463],\n        [0.44743906, 0.21705089, 0.43603421, 0.42565838, 0.44582636],\n        [0.45412702, 0.13048604, 0.443286  , 0.42929145, 0.44886235],\n        [0.46043789, 0.17289293, 0.45381106, 0.43770215, 0.46427926],\n        [0.45981679, 0.16157763, 0.46162493, 0.43962367, 0.46400837],\n        [0.44242588, 0.2049957 , 0.45478091, 0.43299803, 0.4449912 ],\n        [0.44645194, 0.16716787, 0.42856198, 0.42490236, 0.4368877 ],\n        [0.4596837 , 0.15477727, 0.4500529 , 0.43478308, 0.45904246],\n        [0.47382491, 0.29036358, 0.46768648, 0.45924861, 0.47620875],\n        [0.48328566, 0.1646572 , 0.46765341, 0.45668655, 0.47968488],\n        [0.50271734, 0.29206696, 0.49072033, 0.47776052, 0.49476319],\n        [0.52831572, 0.51012244, 0.50427616, 0.50596413, 0.51154576],\n        [0.51633727, 0.46566525, 0.53006525, 0.50628963, 0.51678252],\n        [0.51279919, 0.21113017, 0.51319213, 0.49173633, 0.51771929],\n        [0.56460593, 0.4340508 , 0.52609769, 0.53446177, 0.53837297],\n        [0.57909096, 0.47480364, 0.58204022, 0.55733124, 0.57550451],\n        [0.55755197, 0.28362734, 0.56605979, 0.54728257, 0.56635137],\n        [0.5519953 , 0.35544382, 0.52637321, 0.52273303, 0.53856485],\n        [0.54867905, 0.22213965, 0.54164828, 0.52400352, 0.55096835],\n        [0.58938358, 0.29077463, 0.56659981, 0.55855981, 0.5794659 ],\n        [0.61418337, 0.51381693, 0.6124912 , 0.60677686, 0.61410316],\n        [0.61711145, 0.321151  , 0.60407115, 0.59365157, 0.610153  ],\n        [0.60892617, 0.23405179, 0.61110254, 0.58510436, 0.60864069],\n        [0.60138421, 0.22179437, 0.57953843, 0.57521316, 0.59173403],\n        [0.61123315, 0.19187673, 0.60874404, 0.58820194, 0.61295202],\n        [0.62686054, 0.29074174, 0.61667916, 0.60184173, 0.62350456],\n        [0.69320779, 0.47495984, 0.67920118, 0.66597716, 0.68057874],\n        [0.70403276, 0.25650646, 0.68755511, 0.66820321, 0.69696628],\n        [0.84758547, 0.77465912, 0.7246848 , 0.8080033 , 0.74323959],\n        [0.96632729, 1.        , 0.95532096, 1.        , 0.92422463],\n        [0.79734259, 0.79422989, 0.88952567, 0.87083665, 0.77776395],\n        [0.81315858, 0.65376692, 0.75359282, 0.81817803, 0.75845333],\n        [0.81217145, 0.27860764, 0.7873501 , 0.79079341, 0.80698387],\n        [0.83791403, 0.40398847, 0.86389085, 0.84354656, 0.83226492],\n        [0.84135225, 0.19038051, 0.82949433, 0.80524173, 0.83858515],\n        [0.8334886 , 0.19572413, 0.83950141, 0.81179387, 0.84464582],\n        [0.87420423, 0.43029712, 0.79979284, 0.84145702, 0.81262695],\n        [0.86980107, 0.25608555, 0.849806  , 0.83617537, 0.86962212],\n        [0.93454009, 0.26739756, 0.90973811, 0.88555799, 0.9225091 ],\n        [1.        , 0.41410187, 1.        , 0.97457898, 1.        ]]),\n 'X_train': array([[[0.14409619, 0.12047951, 0.14582967, ..., 0.17822034,\n          0.17204628, 0.17894226],\n         [0.17060402, 0.14136563, 0.17525569, ..., 0.18170296,\n          0.17850393, 0.18579297],\n         [0.20048357, 0.21131431, 0.19088344, ..., 0.21228619,\n          0.21182116, 0.21820686],\n         [0.25753644, 0.53540019, 0.23569476, ..., 0.24969141,\n          0.23769373, 0.24951469],\n         [0.24246357, 0.14290623, 0.2428584 , ..., 0.23541924,\n          0.22393842, 0.22040766]],\n \n        [[0.33613939, 0.09456218, 0.33001233, ..., 0.31818683,\n          0.30878031, 0.32578666],\n         [0.31208269, 0.0918246 , 0.32083186, ..., 0.38508419,\n          0.36790711, 0.37329015],\n         [0.37675517, 0.18798822, 0.37235496, ..., 0.33393583,\n          0.32623169, 0.34210645],\n         [0.34052041, 0.05728515, 0.3356551 , ..., 0.26886793,\n          0.26431182, 0.267787  ],\n         [0.29912824, 0.23289921, 0.28337155, ..., 0.31030684,\n          0.29815405, 0.31645297]],\n \n        [[0.01400812, 0.02622315, 0.01314803, ..., 0.01267413,\n          0.01190726, 0.01385942],\n         [0.01293228, 0.02780157, 0.01277332, ..., 0.01351173,\n          0.01249528, 0.0146946 ],\n         [0.01371975, 0.01523998, 0.01347866, ..., 0.01418401,\n          0.01325129, 0.01519119],\n         [0.0135312 , 0.01055733, 0.01341254, ..., 0.00767061,\n          0.00820068, 0.00916437],\n         [0.0102704 , 0.01997524, 0.00882781, ..., 0.00768163,\n          0.00752867, 0.00756174]],\n \n        ...,\n \n        [[0.14018101, 0.17477224, 0.13656102, ..., 0.14185109,\n          0.1360935 , 0.14403413],\n         [0.15040705, 0.16150858, 0.14752689, ..., 0.14625949,\n          0.14392667, 0.1509864 ],\n         [0.14610368, 0.08654183, 0.15028214, ..., 0.17753703,\n          0.17404134, 0.17881811],\n         [0.17612743, 0.12406713, 0.17822034, ..., 0.17651208,\n          0.17165778, 0.18172994],\n         [0.18935916, 0.14485295, 0.18170296, ..., 0.21546024,\n          0.20722206, 0.21918875]],\n \n        [[0.25741444, 0.13740314, 0.25459574, ..., 0.31104523,\n          0.30272166, 0.30948942],\n         [0.34640981, 0.4914083 , 0.31032887, ..., 0.34495681,\n          0.34112101, 0.35330231],\n         [0.35640294, 0.09784069, 0.35086405, ..., 0.37291705,\n          0.36142845, 0.3826238 ],\n         [0.35185556, 0.275436  , 0.35710193, ..., 0.35141509,\n          0.33834893, 0.36092048],\n         [0.35905371, 0.14640835, 0.35316743, ..., 0.43439209,\n          0.41619766, 0.43475463]],\n \n        [[0.20368892, 0.08495354, 0.19842179, ..., 0.20742595,\n          0.19911588, 0.20926819],\n         [0.21195182, 0.06471688, 0.20755819, ..., 0.23734791,\n          0.22583898, 0.23829623],\n         [0.23512122, 0.14262672, 0.23446042, ..., 0.24744314,\n          0.23699022, 0.23936843],\n         [0.23674054, 0.08300189, 0.23711646, ..., 0.22044173,\n          0.21412072, 0.22416595],\n         [0.22650341, 0.06799046, 0.22021028, ..., 0.2348792 ,\n          0.22551346, 0.23308202]]]),\n 'X_test': array([[[0.37829686, 0.06871061, 0.37136308, ..., 0.37407424,\n          0.35755386, 0.37607784],\n         [0.36523149, 0.07911668, 0.37091122, ..., 0.31298493,\n          0.30666975, 0.31336057],\n         [0.32196491, 0.14428571, 0.3122906 , ..., 0.32826   ,\n          0.31659248, 0.33443183],\n         [0.33190257, 0.22388249, 0.34113253, ..., 0.31883707,\n          0.3083813 , 0.32737798],\n         [0.32097782, 0.09399164, 0.32000529, ..., 0.35917388,\n          0.34364106, 0.36203783]],\n \n        [[0.20800337, 0.06188065, 0.21023628, ..., 0.1957437 ,\n          0.18671511, 0.19724843],\n         [0.19174375, 0.08740174, 0.19045362, ..., 0.19593105,\n          0.18707211, 0.19719199],\n         [0.18766221, 0.13953236, 0.19058588, ..., 0.21348749,\n          0.2047755 , 0.21897431],\n         [0.21514607, 0.05984351, 0.21413772, ..., 0.22522483,\n          0.21438322, 0.22914314],\n         [0.23929151, 0.18997111, 0.22749515, ..., 0.24229634,\n          0.23287413, 0.24685115]],\n \n        [[0.02011934, 0.00622818, 0.01913243, ..., 0.01556163,\n          0.01504683, 0.01607151],\n         [0.01483996, 0.01872072, 0.0148122 , ..., 0.01646535,\n          0.01552984, 0.0172227 ],\n         [0.01519487, 0.01644517, 0.01535223, ..., 0.01477914,\n          0.01471082, 0.01635366],\n         [0.01507287, 0.01239224, 0.01547346, ..., 0.01736907,\n          0.01661137, 0.01872376],\n         [0.01731328, 0.01308116, 0.01736907, ..., 0.01669679,\n          0.01719938, 0.01785472]],\n \n        ...,\n \n        [[0.00992658, 0.01580722, 0.00944498, ..., 0.00842003,\n          0.0090512 , 0.00905151],\n         [0.00821854, 0.04138585, 0.00880577, ..., 0.00665668,\n          0.00616364, 0.00691842],\n         [0.00746434, 0.01540275, 0.00621584, ..., 0.00938988,\n          0.00877819, 0.01048485],\n         [0.00910583, 0.01477303, 0.00914742, ..., 0.00657953,\n          0.00592213, 0.00766331],\n         [0.00724252, 0.00565107, 0.00661259, ..., 0.01195777,\n          0.01089924, 0.01221164]],\n \n        [[0.16540227, 0.09933361, 0.16295627, ..., 0.18451331,\n          0.17815743, 0.18680872],\n         [0.19169939, 0.1068048 , 0.1879739 , ..., 0.17478178,\n          0.16628167, 0.16536499],\n         [0.17427519, 0.14411964, 0.17297434, ..., 0.16836758,\n          0.16294258, 0.16384135],\n         [0.1745192 , 0.17604648, 0.16405837, ..., 0.15282798,\n          0.15130833, 0.14826645],\n         [0.13749696, 0.36445397, 0.1411237 , ..., 0.1211647 ,\n          0.11636356, 0.11746648]],\n \n        [[0.32192054, 0.08509988, 0.31396578, ..., 0.3280396 ,\n          0.31382041, 0.3270394 ],\n         [0.33496374, 0.11744105, 0.32481042, ..., 0.31504585,\n          0.30594523, 0.32280709],\n         [0.32991728, 0.11635589, 0.31295187, ..., 0.35721214,\n          0.3411105 , 0.35587556],\n         [0.34735256, 0.0959433 , 0.34886925, ..., 0.32702565,\n          0.31702297, 0.32808901],\n         [0.33359951, 0.07353959, 0.33115852, ..., 0.36243608,\n          0.35066571, 0.36975757]]]),\n 'y_train': array([0.22696924, 0.31485548, 0.00794126, ..., 0.2238748 , 0.44743906,\n        0.24119918]),\n 'y_test': array([0.35832169, 0.24809788, 0.02046317, 0.27118964, 0.0098933 ,\n        0.25283379, 0.21638828, 0.01740201, 0.26531133, 0.22532775,\n        0.79734259, 0.02076263, 0.14610368, 0.01398594, 0.01762383,\n        0.22833345, 0.0200417 , 0.0107806 , 0.24523636, 0.25001664,\n        0.3078348 , 0.18588762, 0.15157163, 0.20915686, 0.22708014,\n        0.33713759, 0.21280584, 0.28749364, 0.27508264, 0.19868681,\n        0.16234112, 0.01205608, 0.02512145, 0.37119853, 0.00899492,\n        0.33265679, 0.01925423, 0.24490362, 0.21195182, 0.21419223,\n        0.22197822, 0.2154899 , 0.32779886, 0.31931414, 0.23942459,\n        0.01898804, 0.19338525, 0.38925491, 0.1598678 , 0.21008851,\n        0.29305028, 0.2305184 , 0.01343138, 0.01742419, 0.11808744,\n        0.40776604, 0.14690225, 0.2555733 , 0.26095251, 0.20609569,\n        0.0166589 , 0.00682106, 0.3439254 , 0.36087267, 0.34013221,\n        0.23974625, 0.19035735, 0.25163594, 0.01346465, 0.19184358,\n        0.20137087, 0.00947184, 0.34970388, 0.01581598, 0.31208269,\n        0.20368892, 0.11735543, 0.23758346, 0.21426988, 0.00724252,\n        0.02514363, 0.25705953, 0.30546128, 0.01509505, 0.01858876,\n        0.01728001, 0.01848894, 0.24378342, 0.20824738, 0.39533284,\n        0.25517402, 0.21089816, 0.21728667, 0.1739092 , 0.23512122,\n        0.25603913, 0.26253854, 0.01609325, 0.24061135, 0.00545684,\n        0.40037933, 0.21414788, 0.22588231, 0.01050332, 0.01288791,\n        0.02240412, 0.29676582, 0.25763626, 0.1789113 , 0.0177902 ,\n        0.21729776, 0.24227503, 0.40115569, 0.01281028, 0.31614206,\n        0.0052572 , 0.14837737, 0.01983097, 0.27250949, 0.2353985 ,\n        0.32812052, 0.01790111, 0.26751847, 0.29214083, 0.26505624,\n        0.19382889, 0.02586455, 0.31283688, 0.23939132, 0.22723542,\n        0.35628092, 0.26017614, 0.00992658, 0.01663672, 0.16744305,\n        0.24813115, 0.20944522, 0.01620417, 0.23319137, 0.23763892,\n        0.27189949, 0.32707795, 0.31870411, 0.26045342, 0.27103438,\n        0.2440496 , 0.01091369, 0.55755197, 0.19929683, 0.25392073,\n        0.19101174, 0.14895411, 0.22898782, 0.02181629, 0.29852931,\n        0.01343138, 0.00868437, 0.21791886, 0.27550411, 0.24599055,\n        0.01953151, 0.26752957, 0.37176415, 0.20233579, 0.03905193,\n        0.00910583, 0.25197978, 0.21761939, 0.01777911, 0.24554691,\n        0.02042989, 0.14013664, 0.37657771, 0.01371975, 0.37267364,\n        0.18470088, 0.03050065, 0.28218097, 0.35501655, 0.2504492 ,\n        0.00492447, 0.25641624, 0.37307294, 0.19724496, 0.32774339,\n        0.27294206, 0.24201992, 0.22317607, 0.37665535, 0.34653179,\n        0.20330074, 0.01564961, 0.20440985, 0.02599765, 0.24369469,\n        0.20712718, 0.20055012, 0.17961004, 0.18606508, 0.01973115,\n        0.37576808, 0.25178013, 0.30245559, 0.01488432, 0.01861095,\n        0.02147246, 0.21004414, 0.01945387, 0.35940863, 0.01186753,\n        0.0082629 , 0.22303188, 0.2911426 , 0.28160424, 0.21867307,\n        0.37134268, 0.29384884, 0.38515118, 0.26514497, 0.33950001,\n        0.3510348 , 0.3029547 , 0.2084692 , 0.21261729, 0.24101064,\n        0.00549012, 0.37704355, 0.38907745, 0.32500387, 0.23298064,\n        0.22987512, 0.36112775, 0.13509018, 0.25854574, 0.30548348,\n        0.0853242 , 0.02450034, 0.27946363, 0.36628512, 0.19947428,\n        0.22522792, 0.3987711 , 0.84135225, 0.25348818, 0.30630421,\n        0.27188839, 0.19534837, 0.12362193, 0.23899204, 0.34889421,\n        0.47382491, 0.23412303, 0.21283911, 0.01251081, 0.19383999,\n        0.00902819, 0.23423394, 0.01715801, 0.2154899 , 0.25368781,\n        0.25275616, 0.23290299, 0.37950579, 0.29954971, 0.21577828,\n        0.01655908, 0.27711231, 0.22327589, 0.18647545, 0.02377942,\n        0.01018167, 0.32691155, 0.01735765, 0.12859077, 0.13178501,\n        0.19172158, 0.20388857, 0.19845389, 0.39322553, 0.26149599,\n        0.01722455, 0.01288791, 0.24870788, 0.21257294, 0.17488521,\n        0.19653513, 0.33245713, 0.00952729, 0.18965862, 0.18648655,\n        0.17857856, 0.28582997, 0.19534837, 0.2391584 , 0.02349105,\n        0.27056854, 0.21016614, 0.01576052, 0.16956145, 0.2674741 ,\n        0.10364677, 0.29827424, 0.01320956, 0.33081566, 0.15026286,\n        0.28449902, 0.22973092, 0.34318228, 0.16684412, 0.22723542,\n        0.57909096, 0.23589762, 0.26239437, 0.13270557, 0.34728602,\n        0.25806881, 0.01388612, 0.22817817, 0.00545684, 0.31852665,\n        0.3712318 , 0.26533351, 0.24257448, 0.01789002, 0.11315188,\n        0.01915441, 0.14134559, 0.00920565, 0.26064196, 0.01648144,\n        0.26301547, 0.20706063, 0.01674763, 0.01330938, 0.32725537,\n        0.21948272, 0.01112442, 0.01823385, 0.02410106, 0.25429783,\n        0.20476476, 0.00484683, 0.01602671, 0.24613474, 0.1966904 ,\n        0.20048357, 0.24397197, 0.35905371, 0.38258912, 0.26475677,\n        0.3769659 , 0.17103658, 0.26270492, 0.00756416, 0.32991728,\n        0.0443535 , 0.32192054, 0.25693752, 0.37279566, 0.01608216,\n        0.32478207, 0.01308755, 0.25819081, 0.51279919, 0.3084448 ,\n        0.25978794, 0.2347774 , 0.35723478, 0.01474013, 0.36568623,\n        0.0052572 , 0.01990861, 0.23206007, 0.22492847, 0.35158935,\n        0.2356647 , 0.0086511 , 0.31804974, 0.01513942, 0.01932078,\n        0.26390276, 0.61123315, 0.36585259, 0.27315276, 0.22605976,\n        0.25611677, 0.01917659, 0.34640981, 0.20749319, 0.39718508,\n        0.35626982, 0.27870944, 0.01933187, 0.27308622, 0.24677803,\n        0.27061291, 0.02010825, 0.21719794, 0.135312  , 0.43232182,\n        0.23178279, 0.2315277 , 0.26122979, 0.2632151 , 0.0051352 ,\n        0.21909452, 0.2265145 , 0.08479182, 0.07985626, 0.01310974,\n        0.23028548, 0.24136554, 0.21301657, 0.19670149, 0.96632729,\n        0.20928995, 0.16723232, 0.35662474, 0.17060402, 0.23281428,\n        0.20579624, 0.35414034, 0.20951177, 0.37451477, 0.33798053,\n        0.02039662, 0.11584703, 0.03288525, 0.11612431, 0.01813403,\n        0.2813713 , 0.30471819, 0.09714736, 0.07657328, 0.01477341,\n        0.11566957, 0.32767684, 0.01437413, 0.01525033, 0.35536037,\n        0.69320779, 0.1429427 , 0.29719839, 0.22769016, 0.25253434,\n        0.02060735, 0.01672545, 0.00982676, 0.24684457, 0.0904483 ,\n        0.09887757, 0.01132406, 0.07923515, 0.36769372, 0.01013731,\n        0.27153346, 0.2154899 , 0.2587121 , 0.28108296, 0.15906924,\n        0.27009163, 0.36622968, 0.37686609, 0.20983342, 0.40372885,\n        0.22692487, 0.52831572, 0.20877975, 0.20161488, 0.00491338,\n        0.25977686, 0.23820458, 0.26605443, 0.01268827, 0.11789889,\n        0.37239636])}"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_data('TSLA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(input_length, units=256, cell=LSTM, n_layers=2, dropout=0.3,\n",
    "                loss=\"mean_absolute_error\", optimizer=\"rmsprop\"):\n",
    "    model = Sequential()\n",
    "    for i in range(n_layers):\n",
    "        if i == 0:\n",
    "            # first layer\n",
    "            model.add(cell(units, return_sequences=True, input_shape=(None, input_length)))\n",
    "        elif i == n_layers - 1:\n",
    "            # last layer\n",
    "            model.add(cell(units, return_sequences=False))\n",
    "        else:\n",
    "            # hidden layers\n",
    "            model.add(cell(units, return_sequences=True))\n",
    "        # add dropout after each layer\n",
    "        model.add(Dropout(dropout))\n",
    "    model.add(Dense(1, activation=\"linear\"))\n",
    "    model.compile(loss=loss, metrics=[\"mean_absolute_error\"], optimizer=optimizer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Window size or the sequence length\n",
    "N_STEPS = 50\n",
    "# Lookup step, 1 is the next day\n",
    "LOOKUP_STEP = 1\n",
    "# test ratio size, 0.2 is 20%\n",
    "TEST_SIZE = 0.2\n",
    "# features to use\n",
    "FEATURE_COLUMNS = [\"adjclose\", \"volume\", \"open\", \"high\", \"low\"]\n",
    "# date now\n",
    "date_now = time.strftime(\"%Y-%m-%d\")\n",
    "### model parameters\n",
    "N_LAYERS = 3\n",
    "# LSTM cell\n",
    "CELL = LSTM\n",
    "# 256 LSTM neurons\n",
    "UNITS = 256\n",
    "# 40% dropout\n",
    "DROPOUT = 0.4\n",
    "### training parameters\n",
    "# mean squared error loss\n",
    "LOSS = \"mse\"\n",
    "OPTIMIZER = \"rmsprop\"\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 300\n",
    "# Apple stock market\n",
    "ticker = \"AAPL\"\n",
    "ticker_data_filename = os.path.join(\"data\", f\"{ticker}_{date_now}.csv\")\n",
    "# model name to save\n",
    "model_name = f\"{date_now}_{ticker}-{LOSS}-{CELL.__name__}-seq-{N_STEPS}-step-{LOOKUP_STEP}-layers-{N_LAYERS}-units-{UNITS}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create these folders if they does not exist\n",
    "if not os.path.isdir(\"results\"):\n",
    "    os.mkdir(\"results\")\n",
    "if not os.path.isdir(\"logs\"):\n",
    "    os.mkdir(\"logs\")\n",
    "if not os.path.isdir(\"data\"):\n",
    "    os.mkdir(\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "=================>.] - ETA: 0s - loss: 1.9225e-04 - mean_absolute_error: 0.0081\nEpoch 00241: val_loss did not improve from 0.00002\n7862/7862 [==============================] - 5s 685us/sample - loss: 1.9164e-04 - mean_absolute_error: 0.0081 - val_loss: 6.1518e-05 - val_mean_absolute_error: 0.0038\nEpoch 242/300\n7744/7862 [============================>.] - ETA: 0s - loss: 1.8592e-04 - mean_absolute_error: 0.0080\nEpoch 00242: val_loss did not improve from 0.00002\n7862/7862 [==============================] - 5s 670us/sample - loss: 1.8707e-04 - mean_absolute_error: 0.0080 - val_loss: 6.8117e-05 - val_mean_absolute_error: 0.0052\nEpoch 243/300\n7744/7862 [============================>.] - ETA: 0s - loss: 1.8023e-04 - mean_absolute_error: 0.0081\nEpoch 00243: val_loss did not improve from 0.00002\n7862/7862 [==============================] - 5s 689us/sample - loss: 1.8034e-04 - mean_absolute_error: 0.0081 - val_loss: 3.9672e-05 - val_mean_absolute_error: 0.0037\nEpoch 244/300\n7744/7862 [============================>.] - ETA: 0s - loss: 1.8335e-04 - mean_absolute_error: 0.0081\nEpoch 00244: val_loss did not improve from 0.00002\n7862/7862 [==============================] - 6s 719us/sample - loss: 1.8262e-04 - mean_absolute_error: 0.0081 - val_loss: 3.7196e-05 - val_mean_absolute_error: 0.0049\nEpoch 245/300\n7808/7862 [============================>.] - ETA: 0s - loss: 1.7867e-04 - mean_absolute_error: 0.0079\nEpoch 00245: val_loss did not improve from 0.00002\n7862/7862 [==============================] - 6s 726us/sample - loss: 1.7859e-04 - mean_absolute_error: 0.0079 - val_loss: 9.4440e-05 - val_mean_absolute_error: 0.0046\nEpoch 246/300\n7808/7862 [============================>.] - ETA: 0s - loss: 1.6622e-04 - mean_absolute_error: 0.0078\nEpoch 00246: val_loss did not improve from 0.00002\n7862/7862 [==============================] - 5s 699us/sample - loss: 1.6554e-04 - mean_absolute_error: 0.0078 - val_loss: 4.9195e-05 - val_mean_absolute_error: 0.0051\nEpoch 247/300\n7744/7862 [============================>.] - ETA: 0s - loss: 1.9753e-04 - mean_absolute_error: 0.0082\nEpoch 00247: val_loss did not improve from 0.00002\n7862/7862 [==============================] - 6s 715us/sample - loss: 1.9628e-04 - mean_absolute_error: 0.0082 - val_loss: 5.3120e-05 - val_mean_absolute_error: 0.0058\nEpoch 248/300\n7744/7862 [============================>.] - ETA: 0s - loss: 1.8801e-04 - mean_absolute_error: 0.0081\nEpoch 00248: val_loss did not improve from 0.00002\n7862/7862 [==============================] - 6s 709us/sample - loss: 1.8658e-04 - mean_absolute_error: 0.0081 - val_loss: 3.1814e-05 - val_mean_absolute_error: 0.0038\nEpoch 249/300\n7744/7862 [============================>.] - ETA: 0s - loss: 1.8985e-04 - mean_absolute_error: 0.0080\nEpoch 00249: val_loss did not improve from 0.00002\n7862/7862 [==============================] - 5s 695us/sample - loss: 1.8851e-04 - mean_absolute_error: 0.0080 - val_loss: 2.8025e-05 - val_mean_absolute_error: 0.0038\nEpoch 250/300\n7808/7862 [============================>.] - ETA: 0s - loss: 1.8808e-04 - mean_absolute_error: 0.0081\nEpoch 00250: val_loss did not improve from 0.00002\n7862/7862 [==============================] - 6s 700us/sample - loss: 1.8766e-04 - mean_absolute_error: 0.0081 - val_loss: 3.0442e-05 - val_mean_absolute_error: 0.0031\nEpoch 251/300\n7744/7862 [============================>.] - ETA: 0s - loss: 1.8286e-04 - mean_absolute_error: 0.0081\nEpoch 00251: val_loss did not improve from 0.00002\n7862/7862 [==============================] - 5s 698us/sample - loss: 1.8122e-04 - mean_absolute_error: 0.0080 - val_loss: 4.1232e-05 - val_mean_absolute_error: 0.0028\nEpoch 252/300\n7808/7862 [============================>.] - ETA: 0s - loss: 1.8973e-04 - mean_absolute_error: 0.0081\nEpoch 00252: val_loss did not improve from 0.00002\n7862/7862 [==============================] - 5s 697us/sample - loss: 1.8915e-04 - mean_absolute_error: 0.0081 - val_loss: 3.9145e-05 - val_mean_absolute_error: 0.0044\nEpoch 253/300\n7744/7862 [============================>.] - ETA: 0s - loss: 1.8806e-04 - mean_absolute_error: 0.0080\nEpoch 00253: val_loss did not improve from 0.00002\n7862/7862 [==============================] - 6s 700us/sample - loss: 1.8743e-04 - mean_absolute_error: 0.0080 - val_loss: 7.9230e-05 - val_mean_absolute_error: 0.0058\nEpoch 254/300\n7808/7862 [============================>.] - ETA: 0s - loss: 1.7180e-04 - mean_absolute_error: 0.0079\nEpoch 00254: val_loss did not improve from 0.00002\n7862/7862 [==============================] - 6s 737us/sample - loss: 1.7153e-04 - mean_absolute_error: 0.0079 - val_loss: 1.7195e-04 - val_mean_absolute_error: 0.0081\nEpoch 255/300\n7744/7862 [============================>.] - ETA: 0s - loss: 1.7367e-04 - mean_absolute_error: 0.0080\nEpoch 00255: val_loss did not improve from 0.00002\n7862/7862 [==============================] - 6s 734us/sample - loss: 1.7496e-04 - mean_absolute_error: 0.0080 - val_loss: 5.6641e-05 - val_mean_absolute_error: 0.0058\nEpoch 256/300\n7744/7862 [============================>.] - ETA: 0s - loss: 1.7111e-04 - mean_absolute_error: 0.0079\nEpoch 00256: val_loss did not improve from 0.00002\n7862/7862 [==============================] - 6s 714us/sample - loss: 1.7165e-04 - mean_absolute_error: 0.0079 - val_loss: 4.0181e-04 - val_mean_absolute_error: 0.0086\nEpoch 257/300\n7808/7862 [============================>.] - ETA: 0s - loss: 1.7194e-04 - mean_absolute_error: 0.0079\nEpoch 00257: val_loss did not improve from 0.00002\n7862/7862 [==============================] - 6s 729us/sample - loss: 1.7221e-04 - mean_absolute_error: 0.0079 - val_loss: 7.3089e-05 - val_mean_absolute_error: 0.0068\nEpoch 258/300\n7744/7862 [============================>.] - ETA: 0s - loss: 1.8819e-04 - mean_absolute_error: 0.0080\nEpoch 00258: val_loss did not improve from 0.00002\n7862/7862 [==============================] - 6s 750us/sample - loss: 1.9272e-04 - mean_absolute_error: 0.0081 - val_loss: 6.3455e-05 - val_mean_absolute_error: 0.0043\nEpoch 259/300\n7808/7862 [============================>.] - ETA: 0s - loss: 1.8296e-04 - mean_absolute_error: 0.0081\nEpoch 00259: val_loss did not improve from 0.00002\n7862/7862 [==============================] - 6s 750us/sample - loss: 1.8308e-04 - mean_absolute_error: 0.0081 - val_loss: 3.4204e-05 - val_mean_absolute_error: 0.0046\nEpoch 260/300\n7744/7862 [============================>.] - ETA: 0s - loss: 1.7824e-04 - mean_absolute_error: 0.0080\nEpoch 00260: val_loss did not improve from 0.00002\n7862/7862 [==============================] - 6s 747us/sample - loss: 1.7725e-04 - mean_absolute_error: 0.0080 - val_loss: 7.9111e-05 - val_mean_absolute_error: 0.0080\nEpoch 261/300\n7744/7862 [============================>.] - ETA: 0s - loss: 1.8310e-04 - mean_absolute_error: 0.0081\nEpoch 00261: val_loss did not improve from 0.00002\n7862/7862 [==============================] - 6s 777us/sample - loss: 1.8390e-04 - mean_absolute_error: 0.0081 - val_loss: 5.3099e-05 - val_mean_absolute_error: 0.0058\nEpoch 262/300\n7808/7862 [============================>.] - ETA: 0s - loss: 1.6731e-04 - mean_absolute_error: 0.0080\nEpoch 00262: val_loss did not improve from 0.00002\n7862/7862 [==============================] - 6s 713us/sample - loss: 1.6658e-04 - mean_absolute_error: 0.0080 - val_loss: 3.2844e-05 - val_mean_absolute_error: 0.0034\nEpoch 263/300\n7808/7862 [============================>.] - ETA: 0s - loss: 1.8479e-04 - mean_absolute_error: 0.0079\nEpoch 00263: val_loss did not improve from 0.00002\n7862/7862 [==============================] - 6s 821us/sample - loss: 1.8403e-04 - mean_absolute_error: 0.0079 - val_loss: 4.2760e-05 - val_mean_absolute_error: 0.0029\nEpoch 264/300\n7808/7862 [============================>.] - ETA: 0s - loss: 1.8279e-04 - mean_absolute_error: 0.0080\nEpoch 00264: val_loss did not improve from 0.00002\n7862/7862 [==============================] - 8s 956us/sample - loss: 1.8279e-04 - mean_absolute_error: 0.0080 - val_loss: 5.9606e-05 - val_mean_absolute_error: 0.0045\nEpoch 265/300\n7744/7862 [============================>.] - ETA: 0s - loss: 1.7081e-04 - mean_absolute_error: 0.0078\nEpoch 00265: val_loss did not improve from 0.00002\n7862/7862 [==============================] - 6s 711us/sample - loss: 1.7009e-04 - mean_absolute_error: 0.0078 - val_loss: 2.6826e-05 - val_mean_absolute_error: 0.0029\nEpoch 266/300\n7744/7862 [============================>.] - ETA: 0s - loss: 2.0301e-04 - mean_absolute_error: 0.0080\nEpoch 00266: val_loss did not improve from 0.00002\n7862/7862 [==============================] - 6s 738us/sample - loss: 2.0320e-04 - mean_absolute_error: 0.0080 - val_loss: 3.3902e-05 - val_mean_absolute_error: 0.0031\nEpoch 267/300\n7808/7862 [============================>.] - ETA: 0s - loss: 1.7812e-04 - mean_absolute_error: 0.0081\nEpoch 00267: val_loss did not improve from 0.00002\n7862/7862 [==============================] - 6s 764us/sample - loss: 1.7907e-04 - mean_absolute_error: 0.0081 - val_loss: 7.8326e-05 - val_mean_absolute_error: 0.0070\nEpoch 268/300\n7808/7862 [============================>.] - ETA: 0s - loss: 1.7180e-04 - mean_absolute_error: 0.0078\nEpoch 00268: val_loss did not improve from 0.00002\n7862/7862 [==============================] - 7s 932us/sample - loss: 1.7145e-04 - mean_absolute_error: 0.0078 - val_loss: 5.3815e-05 - val_mean_absolute_error: 0.0062\nEpoch 269/300\n7808/7862 [============================>.] - ETA: 0s - loss: 1.7379e-04 - mean_absolute_error: 0.0078\nEpoch 00269: val_loss did not improve from 0.00002\n7862/7862 [==============================] - 8s 1ms/sample - loss: 1.7361e-04 - mean_absolute_error: 0.0078 - val_loss: 4.9605e-05 - val_mean_absolute_error: 0.0059\nEpoch 270/300\n7808/7862 [============================>.] - ETA: 0s - loss: 1.7652e-04 - mean_absolute_error: 0.0079\nEpoch 00270: val_loss did not improve from 0.00002\n7862/7862 [==============================] - 8s 1ms/sample - loss: 1.7589e-04 - mean_absolute_error: 0.0079 - val_loss: 4.2799e-05 - val_mean_absolute_error: 0.0036\nEpoch 271/300\n7808/7862 [============================>.] - ETA: 0s - loss: 1.8005e-04 - mean_absolute_error: 0.0080\nEpoch 00271: val_loss did not improve from 0.00002\n7862/7862 [==============================] - 6s 767us/sample - loss: 1.8003e-04 - mean_absolute_error: 0.0080 - val_loss: 5.6402e-05 - val_mean_absolute_error: 0.0052\nEpoch 272/300\n7808/7862 [============================>.] - ETA: 0s - loss: 1.8006e-04 - mean_absolute_error: 0.0080\nEpoch 00272: val_loss did not improve from 0.00002\n7862/7862 [==============================] - 6s 734us/sample - loss: 1.8086e-04 - mean_absolute_error: 0.0080 - val_loss: 6.9863e-05 - val_mean_absolute_error: 0.0041\nEpoch 273/300\n7744/7862 [============================>.] - ETA: 0s - loss: 1.7780e-04 - mean_absolute_error: 0.0079\nEpoch 00273: val_loss did not improve from 0.00002\n7862/7862 [==============================] - 6s 730us/sample - loss: 1.7646e-04 - mean_absolute_error: 0.0079 - val_loss: 5.4635e-05 - val_mean_absolute_error: 0.0045\nEpoch 274/300\n7744/7862 [============================>.] - ETA: 0s - loss: 1.8416e-04 - mean_absolute_error: 0.0079\nEpoch 00274: val_loss did not improve from 0.00002\n7862/7862 [==============================] - 5s 699us/sample - loss: 1.8403e-04 - mean_absolute_error: 0.0079 - val_loss: 3.3542e-05 - val_mean_absolute_error: 0.0028\nEpoch 275/300\n7808/7862 [============================>.] - ETA: 0s - loss: 1.8418e-04 - mean_absolute_error: 0.0080\nEpoch 00275: val_loss did not improve from 0.00002\n7862/7862 [==============================] - 5s 686us/sample - loss: 1.8535e-04 - mean_absolute_error: 0.0080 - val_loss: 5.4465e-05 - val_mean_absolute_error: 0.0053\nEpoch 276/300\n7744/7862 [============================>.] - ETA: 0s - loss: 1.8403e-04 - mean_absolute_error: 0.0080\nEpoch 00276: val_loss did not improve from 0.00002\n7862/7862 [==============================] - 5s 693us/sample - loss: 1.8290e-04 - mean_absolute_error: 0.0080 - val_loss: 3.5923e-05 - val_mean_absolute_error: 0.0034\nEpoch 277/300\n7744/7862 [============================>.] - ETA: 0s - loss: 1.6823e-04 - mean_absolute_error: 0.0078\nEpoch 00277: val_loss did not improve from 0.00002\n7862/7862 [==============================] - 5s 697us/sample - loss: 1.6753e-04 - mean_absolute_error: 0.0078 - val_loss: 3.3606e-05 - val_mean_absolute_error: 0.0027\nEpoch 278/300\n7744/7862 [============================>.] - ETA: 0s - loss: 1.8712e-04 - mean_absolute_error: 0.0079\nEpoch 00278: val_loss did not improve from 0.00002\n7862/7862 [==============================] - 5s 690us/sample - loss: 1.8704e-04 - mean_absolute_error: 0.0078 - val_loss: 1.0231e-04 - val_mean_absolute_error: 0.0053\nEpoch 279/300\n7744/7862 [============================>.] - ETA: 0s - loss: 1.8209e-04 - mean_absolute_error: 0.0077\nEpoch 00279: val_loss did not improve from 0.00002\n7862/7862 [==============================] - 5s 697us/sample - loss: 1.8177e-04 - mean_absolute_error: 0.0077 - val_loss: 3.6627e-05 - val_mean_absolute_error: 0.0038\nEpoch 280/300\n7744/7862 [============================>.] - ETA: 0s - loss: 1.8552e-04 - mean_absolute_error: 0.0080\nEpoch 00280: val_loss did not improve from 0.00002\n7862/7862 [==============================] - 5s 694us/sample - loss: 1.8593e-04 - mean_absolute_error: 0.0080 - val_loss: 2.5969e-05 - val_mean_absolute_error: 0.0031\nEpoch 281/300\n7744/7862 [============================>.] - ETA: 0s - loss: 1.8400e-04 - mean_absolute_error: 0.0078\nEpoch 00281: val_loss did not improve from 0.00002\n7862/7862 [==============================] - 5s 693us/sample - loss: 1.8312e-04 - mean_absolute_error: 0.0077 - val_loss: 1.0368e-04 - val_mean_absolute_error: 0.0072\nEpoch 282/300\n7744/7862 [============================>.] - ETA: 0s - loss: 1.6956e-04 - mean_absolute_error: 0.0078\nEpoch 00282: val_loss did not improve from 0.00002\n7862/7862 [==============================] - 5s 695us/sample - loss: 1.6849e-04 - mean_absolute_error: 0.0078 - val_loss: 6.2522e-05 - val_mean_absolute_error: 0.0042\nEpoch 283/300\n7744/7862 [============================>.] - ETA: 0s - loss: 1.7128e-04 - mean_absolute_error: 0.0077\nEpoch 00283: val_loss did not improve from 0.00002\n7862/7862 [==============================] - 5s 690us/sample - loss: 1.7005e-04 - mean_absolute_error: 0.0077 - val_loss: 4.4096e-05 - val_mean_absolute_error: 0.0057\nEpoch 284/300\n7808/7862 [============================>.] - ETA: 0s - loss: 1.8432e-04 - mean_absolute_error: 0.0078\nEpoch 00284: val_loss did not improve from 0.00002\n7862/7862 [==============================] - 5s 699us/sample - loss: 1.8362e-04 - mean_absolute_error: 0.0078 - val_loss: 2.7774e-05 - val_mean_absolute_error: 0.0036\nEpoch 285/300\n7744/7862 [============================>.] - ETA: 0s - loss: 1.6452e-04 - mean_absolute_error: 0.0077\nEpoch 00285: val_loss did not improve from 0.00002\n7862/7862 [==============================] - 5s 684us/sample - loss: 1.6561e-04 - mean_absolute_error: 0.0078 - val_loss: 5.2594e-05 - val_mean_absolute_error: 0.0049\nEpoch 286/300\n7744/7862 [============================>.] - ETA: 0s - loss: 1.7434e-04 - mean_absolute_error: 0.0078\nEpoch 00286: val_loss did not improve from 0.00002\n7862/7862 [==============================] - 5s 683us/sample - loss: 1.7499e-04 - mean_absolute_error: 0.0078 - val_loss: 8.2276e-05 - val_mean_absolute_error: 0.0038\nEpoch 287/300\n7744/7862 [============================>.] - ETA: 0s - loss: 1.6765e-04 - mean_absolute_error: 0.0077\nEpoch 00287: val_loss did not improve from 0.00002\n7862/7862 [==============================] - 5s 690us/sample - loss: 1.7010e-04 - mean_absolute_error: 0.0077 - val_loss: 1.3408e-04 - val_mean_absolute_error: 0.0053\nEpoch 288/300\n7744/7862 [============================>.] - ETA: 0s - loss: 1.8516e-04 - mean_absolute_error: 0.0079\nEpoch 00288: val_loss did not improve from 0.00002\n7862/7862 [==============================] - 5s 684us/sample - loss: 1.8375e-04 - mean_absolute_error: 0.0078 - val_loss: 3.2260e-05 - val_mean_absolute_error: 0.0033\nEpoch 289/300\n7744/7862 [============================>.] - ETA: 0s - loss: 1.6677e-04 - mean_absolute_error: 0.0077\nEpoch 00289: val_loss did not improve from 0.00002\n7862/7862 [==============================] - 5s 688us/sample - loss: 1.6696e-04 - mean_absolute_error: 0.0077 - val_loss: 6.7339e-05 - val_mean_absolute_error: 0.0064\nEpoch 290/300\n7744/7862 [============================>.] - ETA: 0s - loss: 1.7991e-04 - mean_absolute_error: 0.0079\nEpoch 00290: val_loss did not improve from 0.00002\n7862/7862 [==============================] - 5s 690us/sample - loss: 1.8050e-04 - mean_absolute_error: 0.0079 - val_loss: 1.3620e-04 - val_mean_absolute_error: 0.0058\nEpoch 291/300\n7744/7862 [============================>.] - ETA: 0s - loss: 1.6551e-04 - mean_absolute_error: 0.0076\nEpoch 00291: val_loss did not improve from 0.00002\n7862/7862 [==============================] - 5s 688us/sample - loss: 1.6561e-04 - mean_absolute_error: 0.0076 - val_loss: 7.4739e-05 - val_mean_absolute_error: 0.0076\nEpoch 292/300\n7744/7862 [============================>.] - ETA: 0s - loss: 1.8172e-04 - mean_absolute_error: 0.0079\nEpoch 00292: val_loss did not improve from 0.00002\n7862/7862 [==============================] - 5s 688us/sample - loss: 1.8013e-04 - mean_absolute_error: 0.0079 - val_loss: 4.8500e-05 - val_mean_absolute_error: 0.0049\nEpoch 293/300\n7744/7862 [============================>.] - ETA: 0s - loss: 1.7277e-04 - mean_absolute_error: 0.0076\nEpoch 00293: val_loss did not improve from 0.00002\n7862/7862 [==============================] - 6s 703us/sample - loss: 1.7263e-04 - mean_absolute_error: 0.0077 - val_loss: 1.4057e-04 - val_mean_absolute_error: 0.0095\nEpoch 294/300\n7744/7862 [============================>.] - ETA: 0s - loss: 1.8647e-04 - mean_absolute_error: 0.0081\nEpoch 00294: val_loss did not improve from 0.00002\n7862/7862 [==============================] - 5s 688us/sample - loss: 1.8651e-04 - mean_absolute_error: 0.0081 - val_loss: 2.7199e-05 - val_mean_absolute_error: 0.0032\nEpoch 295/300\n7744/7862 [============================>.] - ETA: 0s - loss: 1.5820e-04 - mean_absolute_error: 0.0077\nEpoch 00295: val_loss did not improve from 0.00002\n7862/7862 [==============================] - 5s 686us/sample - loss: 1.5820e-04 - mean_absolute_error: 0.0077 - val_loss: 3.7627e-05 - val_mean_absolute_error: 0.0042\nEpoch 296/300\n7744/7862 [============================>.] - ETA: 0s - loss: 1.6908e-04 - mean_absolute_error: 0.0078\nEpoch 00296: val_loss did not improve from 0.00002\n7862/7862 [==============================] - 5s 690us/sample - loss: 1.6839e-04 - mean_absolute_error: 0.0078 - val_loss: 3.3888e-05 - val_mean_absolute_error: 0.0039\nEpoch 297/300\n7808/7862 [============================>.] - ETA: 0s - loss: 1.6883e-04 - mean_absolute_error: 0.0078\nEpoch 00297: val_loss did not improve from 0.00002\n7862/7862 [==============================] - 5s 691us/sample - loss: 1.6892e-04 - mean_absolute_error: 0.0078 - val_loss: 2.9952e-05 - val_mean_absolute_error: 0.0029\nEpoch 298/300\n7744/7862 [============================>.] - ETA: 0s - loss: 1.6621e-04 - mean_absolute_error: 0.0077\nEpoch 00298: val_loss did not improve from 0.00002\n7862/7862 [==============================] - 5s 689us/sample - loss: 1.6645e-04 - mean_absolute_error: 0.0077 - val_loss: 4.0006e-05 - val_mean_absolute_error: 0.0036\nEpoch 299/300\n7744/7862 [============================>.] - ETA: 0s - loss: 1.8017e-04 - mean_absolute_error: 0.0080\nEpoch 00299: val_loss did not improve from 0.00002\n7862/7862 [==============================] - 5s 691us/sample - loss: 1.7988e-04 - mean_absolute_error: 0.0080 - val_loss: 3.9350e-05 - val_mean_absolute_error: 0.0053\nEpoch 300/300\n7808/7862 [============================>.] - ETA: 0s - loss: 1.7594e-04 - mean_absolute_error: 0.0078\nEpoch 00300: val_loss did not improve from 0.00002\n7862/7862 [==============================] - 5s 686us/sample - loss: 1.7549e-04 - mean_absolute_error: 0.0078 - val_loss: 4.0665e-05 - val_mean_absolute_error: 0.0024\n"
    }
   ],
   "source": [
    "# load the data\n",
    "data = load_data(ticker, N_STEPS, lookup_step=LOOKUP_STEP, test_size=TEST_SIZE, feature_columns=FEATURE_COLUMNS)\n",
    "# construct the model\n",
    "model = create_model(N_STEPS, loss=LOSS, units=UNITS, cell=CELL, n_layers=N_LAYERS,\n",
    "                    dropout=DROPOUT, optimizer=OPTIMIZER)\n",
    "# some tensorflow callbacks\n",
    "checkpointer = ModelCheckpoint(os.path.join(\"results\", model_name), save_best_only=True, verbose=1)\n",
    "tensorboard = TensorBoard(log_dir=os.path.join(\"logs\", model_name))\n",
    "\n",
    "history = model.fit(data[\"X_train\"], data[\"y_train\"],\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    epochs=EPOCHS,\n",
    "                    validation_data=(data[\"X_test\"], data[\"y_test\"]),\n",
    "                    callbacks=[checkpointer, tensorboard],\n",
    "                    verbose=1)\n",
    "model.save(os.path.join(\"results\", model_name) + \".h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}